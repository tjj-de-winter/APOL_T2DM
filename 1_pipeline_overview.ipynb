{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2DM islet scRNA-seq dataset generation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the following pipeline:\n",
    "<ol>\n",
    "<li><b>Retrieval of FASTQ files</b></li>\n",
    "<li><b>Processing of the FASTQ files</b></li>\n",
    "<li><b>Mapping of the FASTQ files</b></li>\n",
    "<li><b>Counting of genes</b></li>\n",
    "<li><b>Quality control and filtering</b></li>\n",
    "<li><b>Gene expression analysis</b></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download fastq files\n",
    "Raw sequencing fastq files were downloaded in several ways depending on the database where they are located.\n",
    "<ol>\n",
    "<li>Sequence Read Archive (SRA) data from NCBI can be retreaved via the <b>SRA toolkit</b></li>\n",
    "<li>ArrayExpress Archive of Functional Genomics Data from EMBL-EBI can be retreaved via <b>ArrayExpress</b></li>   \n",
    "</ol>\n",
    "\n",
    "#### Requiered software\n",
    "<ul><li>Bash</li>\n",
    "<li>SRA toolkit</li>\n",
    "</ul>\n",
    "\n",
    ">### SRA toolkit\n",
    "First go to the GEO website (https://www.ncbi.nlm.nih.gov/geo/) and search for the dataset with the GSE number. Next scroll to the bottom of the page and go to the SRA Run Selector website, here the <b>Accesion list</b> (SRR_Acc_list.txt) can be downloaded.  \n",
    ">\n",
    "><code>prefetch -X <b>100G</b> -p --option-file <b>SRR_Acc_list.txt</b></code>\n",
    ">\n",
    "><code>for srr in &#0036;(awk '{print &#0036;0}' SRR_Acc_list.txt); \n</code>",
    "><code>do\n</code>",
    "><code>fastq-dump  --outdir <b>outdir</b> --split-files --gzip <b>&#0036;{srr}.sra</b>;\n</code>",
    "><code>rm <b>&#0036;{srr}.sra</b>\n</code>",
    "><code>done</code>\n",
    "\n",
    ">### ArrayExpress\n",
    "First go to the ArrayExpress website (https://www.ebi.ac.uk/arrayexpress/) and search for the dataset with the accesion number. Click on the ENA link address under \"Links\", here the FASTQ FTP files (e.g. Download All) can be downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare fastq files \n",
    "Prepare the fastq files for downstream analysis. Below the processing is decribed for <i>SMART-seq</i> datasets.\n",
    "\n",
    "#### Requiered software\n",
    "<ul><li>Python3</li>\n",
    "<li>Bash</li>\n",
    "<li>script: Smartseq_add_cell_ID.py</li>\n",
    "<li>script: merge_files.sh</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n",
    ">### SMART-seq/SMART-seq2\n",
    ">First add a cell ID to sequence identifier line.\n",
    ">SMART-seq or SMART-seq2 datasets don't have a UMI and usually each fastq file corresponds to one cell. First make the following files:\n",
    "><ol><li>Generate a <b>cell ID text file</b> with two columns seperated by a comma, the first column contains the SRR name (the name of the downloaded fastq file; e.g. SRR0001) and the second column a cell ID (e.g. cell_1).</li>\n",
    "><li>Generate a <b>fastq list text file</b> with a single column containing the single-end fastq file names (e.g. read 1) of all the files that need to be used.</ol>\n",
    ">Run the following script:\n",
    "><code>Smartseq_add_cell_ID.py --fq1 <b>fastq list text file</b> --cellid <b>cell ID text file</b> --outdir <b>outdir</b> --outprefix <b>outprefix</b> --idx <b>index sequence</b> </code>\n",
    ">\n",
    ">For downstream analysis merge the single cell fastq files into one fastq file per donor:<br>\n",
    "><code>merge_files.sh &#92; \n",
    "<b>txt file</b> with files to merge (single column) &#92;\n",
    "<b>output file name</b> e.g. merged_fastq.gz &#92;\n",
    "<b>empty</b> <i>or</i> <b>-f</b> to overwrite existing file \n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim galore\n",
    "The trim galore package is used to remove low-quality bases and sequence adapters from the reads in the processed and merged fastq file. To use Trim Galore, make sure that <i>Cutadapt</i> and <i>FastQC</i> are also installed. \n",
    "\n",
    "#### Requiered software\n",
    "<ul><li>Python3</li>\n",
    "<li>Bash</li>\n",
    "<li>Trim_galore</li>\n",
    "<li>Cutadapt</li>\n",
    "<li>FastQC</li>\n",
    "</ul>\n",
    " \n",
    "><code>trim_galore --illumina &#92; \n",
    "-o <b>outdir</b> &#92;\n",
    "<b>fastq file</b></code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate BAM file\n",
    "When the fastq files are processed (e.g. addition of cell barcode, merged, and trimmed) the reads are mapped to a reference genome using the STAR package.  \n",
    "\n",
    "#### Requiered software\n",
    "<ul><li>Bash</li>\n",
    "<li>STAR</li>\n",
    "<li>wget</li>\n",
    "<li>samtools</li>\n",
    "<li>picard</li>\n",
    "<li>java JDK (version 8!)</li>\n",
    "<li>script: align_and_add_RG.sh</li>\n",
    "<li>script: star_allign.sh</li>\n",
    "</ul>\n",
    "\n",
    ">### Index a genome with STAR\n",
    ">First an indexed genome has to be generated, do this for each read length variety. Download a gtf file and primary assembly FASTA file from ensembl genome database, for example download by:<br> \n",
    "><code>wget <b>http://ftp.ensembl.org/pub/release-102/gtf/homo_sapiens/Homo_sapiens.GRCh38.102.gtf.gz</b>\n",
    ">wget <b>http://ftp.ensembl.org/pub/release-102/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz</b></code>\n",
    ">\n",
    ">When making the indexed genome select as <i>sjdbOverhang</i> the read length - 1<br>\n",
    "><code>STAR --runThreadN <b>integer</b> --runMode genomeGenerate --genomeDir <b>outdir</b> --genomeFastaFiles <b>genomeFastaFiles</b> --sjdbGTFfile <b>genomeGTFfile</b> --sjdbOverhang <b>ReadLength - 1</b></code>\n",
    "    \n",
    ">### Map to genome\n",
    ">Next a script is used to map the files to previously generated indexed genome, sort the bam file based on genomic location, filtering out of multimappers, and addition of a read group ID. Make sure that the installed Java JDK version is set to 8 to support the picard package. As library tag use for example a 3 letter code, this tag is used to identify the dataset.\n",
    ">\n",
    "><code>align_and_add_RG.sh &#92; \n",
    "<b>fastq file</b> &#92; \n",
    "<b>library tag</b> &#92; \n",
    "<b>donor/sample ID</b> &#92; \n",
    "<b>indexed genome folder</b>\n",
    "</code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count genes\n",
    "\n",
    "#### Requiered software\n",
    "<ul><li>Bash</li>\n",
    "<li>samtools</li>\n",
    "<li>featureCounts</li>\n",
    "<li>script: count_tables.sh</li>\n",
    "<li>script: split_bam.sh</li>\n",
    "<li>script: count_genes.sh</li>\n",
    "</ul>\n",
    "\n",
    ">### Count genes\n",
    "To count genes of cells sequenced from different datasets, the pipeline counts reads only falling reads within the 3'UTR region or mitochondrial reads. In order to do this we made a GTF file that contains the longest possible 3' UTR region as well as all the mitochondrial genes (both annotated as three_prime_utr_or_mitogene). First BAM files are split in BAM files specific for each cell, using the cell ID annotation from the read ID. After this the reads are count with featureCounts with the count feature set as three_prime_utr_or_mitogene. The output file is a tsv file and the gene is annotated with the chromosome number separated by a > (e.g INS>11).\n",
    "\n",
    "><code>count_tables.sh &#92;\n",
    "<b>BAM + dir</b> &#92;\n",
    "<b>GTF</b> &#92;\n",
    "<b>count feature: three_prime_utr_or_mitogene or gene</b> &#92;\n",
    "<b>outdir</b>\n",
    "</code>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count matrix quality control and filtering\n",
    "A script is used after generation of the gene count matrix to make plots to assess the quality of the data. This script makes the following plots and histograms: number of reads, frequency of genes, types of reads per cells, mitochondrial read fraction, GAPDH read fraction, and reads per gene. The output is used to manually set treshold criteria to filter cells and genes in the jupyter python notebook.\n",
    "\n",
    "#### Requiered software\n",
    "<ul><ul><li>Bash</li>\n",
    "<li>Python 3</li>\n",
    "<li>script: count_matrix_QC.py</li>\n",
    "<li>notebook: T2DM-ND_dataset_generation.ipynb</li>\n",
    "</ul>\n",
    "\n",
    ">### Quality control\n",
    "\n",
    "><code>count_matrix_QC.py &#92;\n",
    "<b>TSV count matrix</b> &#92;\n",
    "<b>sample ID</b> &#92;\n",
    "<b>outdir</b>\n",
    "</code>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gene expression analysis\n",
    "Differential gene expression analysis and generation of plots is performed in a jupyter notebook.\n",
    "\n",
    "#### Requiered software\n",
    "<ul><li>Python 3</li>\n",
    "<li>notebook: APOLgene_expression_T2DM.ipynb</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
